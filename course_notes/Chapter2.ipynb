{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68b0f21",
   "metadata": {},
   "source": [
    "# Chapter 2 \n",
    "> Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11375a91",
   "metadata": {},
   "source": [
    "## Lecture 9\n",
    "\n",
    "### Review of Linear Algebra\n",
    "\n",
    "Reference Books: Matrix Cookbook by Kaare Brandt Petersen & Michael Syskind Pedersen, 2012\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db611cc5",
   "metadata": {},
   "source": [
    "$A \\in \\mathbb{R}^{n \\times m}, n\\text{ rows and } m\\text{ columns}$ \n",
    "\n",
    "range($A$):=span{\\underline{a}$_1$,...,\\underline{a}$_m$}\n",
    "\n",
    "null($A$):={\\underline{x} $\\in \\mathbb{R}^{m}$|$A$\\underline{x}=0}\n",
    "\n",
    "Column rank = Row rank = number of linearly independent vectors.\n",
    "\n",
    "Full rank: rank(A) = min{m,n}.\n",
    "\n",
    "A \\textbf{nonsingular} or \\textbf{ invertible} matrix is a square matrix of full rank\n",
    "\n",
    "Angle between two vectors: $\\displaystyle \\alpha = cos^{-1}(\\frac{\\mathbf{x}^T\\mathbf{y}}{||\\mathbf{x}||\\cdot ||\\mathbf{y}|| })$\n",
    "\n",
    "$Q$ matrix is \\textbf{ unitary} or \\textbf{ orthogonal} if $Q^T=Q^{-1}$:\n",
    "$||Q\\mathbf{x}||=||\\mathbf{x}||$ rotation\n",
    "\n",
    "\\textbf{ Norms}:\n",
    "\n",
    "$\\displaystyle ||\\mathbf{x}||_p:=(\\sum_{j=1}^n|\\mathbf{x}_j|^p)^{1/p}$\n",
    "\n",
    "$\\displaystyle ||\\mathbf{x}||_1:=\\sum_{j=1}^n|\\mathbf{x}_j|$\n",
    "\n",
    "$\\displaystyle ||\\mathbf{x}||_2:=(\\mathbf{x}^T\\mathbf{x})^{1/2}$\n",
    "\n",
    "$\\displaystyle ||\\mathbf{x}||_\\infty:=\\max_{1\\le j\\le n}|\\mathbf{x}_j|$\n",
    "\n",
    "$\\displaystyle \n",
    "||A||_{(m,n)}:=\\sup_{x\\in \\mathbf{R}^m, \\mathbf{x} \\ne 0}\\frac{||A\\mathbf{x}||_n}{||\\mathbf{x}||_m}=\\sup_{||\\mathbf{x}||_m=1}||A\\mathbf{x}||_n$\n",
    "\n",
    "$\\displaystyle ||A||_1 = \\text{ max column in } A$ <br>\n",
    "$\\displaystyle ||A||_\\infty = \\text{ max row in } A$\n",
    "\n",
    "Frobenins (Hilbert-Schmidt norm): \n",
    "\n",
    "$\\displaystyle ||A||_F = (\\sum_{i=1}^n\\sum_{j=1}^m |a_{i,j}|^2)^{1/2}=\\sqrt{\\text{ Tr}(A^TA)}=\\sqrt{\\text{Tr}(AA^T)}\\text{ , where Tr}(B) = \\sum_j b_{jj}, \\text{ sum of diagonal entries.}$\n",
    "\n",
    "$||QA||_2=||A||_2 \\,\\&\\, ||QA||_F=||A||_F $\n",
    "\n",
    "\\textbf{Singular Value Decomposition} (SVD): rotation & stretching of a basis\n",
    "\n",
    "$U$ - left singular matrix, $V$- right singular matrix, $\\Sigma$ is the diagonal entry matrix.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\text{To find the } V, \\text{we note that } A^TA &= V\\Sigma U^{-1}U\\Sigma V^T \\\\\n",
    "& = V\\Sigma^2V^T \\\\\n",
    "&= V\\left[ \n",
    "\\begin{array}{ccc}\n",
    "\\sigma_1^2 &  & \\\\\n",
    "& \\sigma_2^2 &  \\\\\n",
    "& & ... \\\\\n",
    "& & \\sigma_n^2  \\\\\n",
    "\\end{array}\n",
    "\\right]V^T\n",
    "\\end{align}\n",
    "\n",
    "The eigenvectors of this matrix $A^TA$ will give us the vectors $\\mathbf{v}_i$, and the eigenvalues will give the numbers $\\sigma_i$. \n",
    "\n",
    "Similarly, the matrix $AA^T$ gives us info for $U$. Examples can be seen here [SVD examples](\n",
    "https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf#:~:text=The%20singular%20value%20decomposition%20of%20a%20matrix%20is,its%20eigenvectors%20are%20orthogonal%20and%20we%20can%20write)\n",
    "\n",
    "<div>\n",
    "<img src=\"Screenshot%202022-02-04%20162022.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6478c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da8d99",
   "metadata": {},
   "source": [
    "## Lecture 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c79da4",
   "metadata": {},
   "source": [
    "### PCA (principal component analysis)\n",
    "\n",
    "$\\overline{x} $ center data by subtraction the mean\n",
    "\n",
    "$\\frac{1}{N-1}XX^T$ is simply the empirical approximation to Cov(X) which henceforth we denote as $C_x$. $C_x$ is non-negative definite & Symmetric (NDS). Hence it has an eigen decomposition $C_x = Q\\Lambda Q^T$. The eigen-vectors of $C_x$ are called the principal components or the Rarhumen-Loere modes or PCA modes of $C_x$.\n",
    "\n",
    "At the same time write $X=U\\Sigma V^T$ then, \n",
    "\\begin{align}\n",
    "C_x=\\frac{1}{N-1}XX^T&=\\frac{1}{N-1}U\\Sigma V^TV\\Sigma U^T\\\\\n",
    "&=\\frac{1}{N-1}U\\Sigma^2U^T\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc08a61",
   "metadata": {},
   "source": [
    "Thus, columns of U,\\textbf{ the left singular vectors of X are precisely the Principal Components of} $C_x$\n",
    "\n",
    "But why do we care about the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37c472",
   "metadata": {},
   "source": [
    "A random varibale is called Gaussian if its PDF has the form \n",
    "$$\\Pi(x)=\\frac{1}{\\sqrt{(2\\pi)^ddet(C)}}exp(-\\frac{1}{2}(x-m)^TC^{-1}(x-m))$$, where m is mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c18c7",
   "metadata": {},
   "source": [
    "\\begin{lemma}\n",
    "Suppose $x \\sim \\mathcal{N}(m,C),m \\in \\mathbb{R}^d, C\\in \\mathbb{R}^{d\\times{d}}. \\text{ Let } b\\in \\mathbb{R}^n \\& A\\in \\mathbb{R}^{n\\times d} \\text{ then } z=Ax+b \\text{ is also Gaussian & } z\\in \\mathcal{N}(b+Am, ACA^T)  $\n",
    "\\end{lemma}\n",
    "\n",
    "PCA explains the covariance and directions of maximum singular values in our dataset. It is good for low-dimension compression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28596a1b",
   "metadata": {},
   "source": [
    "## Lecture 11\n",
    "### From PCA & SVD to Proper Orthogonal Decomposition & Dynamic Mode Decoposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95739b2",
   "metadata": {},
   "source": [
    "POD(proper orthogonal decomposition)\n",
    "\n",
    "$\\displaystyle f(x,t):=\\sum_{k=0}^{\\infty}c_k(t)\\psi_k(x)$\n",
    "\n",
    "In the context, $\\psi_k(x)$ are called the POD modes & $c_k(t)$ are called the POD coefficient. Luckily, there is an easy way to compute POD using SVD.\n",
    "\n",
    "Suppose dynamics of f(x,t) are observed over u discrete set, ie (x_j,t_k) ,j=0...N-1, k=0,...,T-1. Now put it into a matrix D:\n",
    "\n",
    "$$D:=\\begin{bmatrix} \n",
    "\\\\\n",
    "\td_0=f(x_j,t_0) \\;\\; | \\;\\; d_1=f(x_j,t_1) \\;\\; | \\;\\; ....\\;\\; |\\;\\; d_{T-1}=f(x_j,t_{T-1})\\\\\n",
    "    \\\\\n",
    "\t\\end{bmatrix}$$\n",
    "    \n",
    "Then, compute the SVD of D.\n",
    "\n",
    "$D=U\\Sigma V^T$, $U$ doesn't change even if the matrix is shuffled around, the direction of time is not important. Then the columns of $U$ are precisely the POD modes & the rows of $\\Sigma V^T$ are precisely the POD coefficients.\n",
    "\n",
    "\\textbf{ The power of this method is also important that it could be used to predict the future behavior based on the DMD}. Book:Data-Driven Science and Engineering by Steven\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "# first load frames of cylinder flow simulation\n",
    "#from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "data = np.load(\n",
    "    \"/content/drive/MyDrive/Courses/AMATH482582-WIN2022/Notebooks/CylData/cyldata.npy\")\n",
    "Nx = 200\n",
    "Ny = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f515bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax0 is the flattened frames and ax1 are the time frames\n",
    "\n",
    "# compute svd of data\n",
    "\n",
    "import numpy.matlib as matlib\n",
    "\n",
    "mean_data = np.mean(data, 1)\n",
    "\n",
    "centered_data = data - np.transpose(matlib.repmat(mean_data, 200, 1))\n",
    "\n",
    "dU, ds, dVt = np.linalg.svd(centered_data, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the singular values and some of the principal modes\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "ax.plot(np.log(ds[0:100]))\n",
    "ax.set_xlabel('index $j$')\n",
    "ax.set_ylabel('$\\log(\\sigma_j)$')\n",
    "#ax.set_xlim(0, 150)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(8, 2, figsize=(20, 20))\n",
    "\n",
    "for j in range(8):\n",
    "\n",
    "    ax[j][0].imshow(np.reshape(dU[:, j], (Ny, Nx)), cmap='bwr')\n",
    "    ax[j][0].get_xaxis().set_visible(False)\n",
    "    ax[j][0].get_yaxis().set_visible(False)\n",
    "    ax[j][0].set_aspect('equal')\n",
    "\n",
    "    ax[j][1].plot(np.abs(dVt[j, :]))\n",
    "    ax[j][1].set_xlabel('Time step')\n",
    "\n",
    "    if j == 0:\n",
    "        ax[j][0].set_title('POD Modes', fontsize=30)\n",
    "        ax[j][1].set_title('|POD Coeff.|', fontsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abaaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate the dynamics using the POD modes only keeping the first 40 and compare to original data.\n",
    "\n",
    "dss = np.copy(ds)\n",
    "\n",
    "dss[10:None] = 0\n",
    "\n",
    "# low rank approx\n",
    "approx_centered_data = np.dot(dU, np.dot(np.diag(dss), dVt))\n",
    "\n",
    "print(approx_centered_data.shape)\n",
    "\n",
    "# add the mean back\n",
    "approx_data = approx_centered_data + \\\n",
    "    np.transpose(matlib.repmat(mean_data, 200, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c51827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side by side comparison of original and approximate dynamics\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(20, 15))\n",
    "\n",
    "frm_indx = [10, 100, 150, 199]\n",
    "\n",
    "for j in range(4):\n",
    "\n",
    "    ax[j][0].imshow(np.reshape(data[:, frm_indx[j]], (Ny, Nx)), cmap='bwr')\n",
    "    ax[j][0].get_xaxis().set_visible(False)\n",
    "    ax[j][0].get_yaxis().set_visible(False)\n",
    "    ax[j][0].set_aspect('equal')\n",
    "\n",
    "    ax[j][1].imshow(np.reshape(\n",
    "        approx_data[:, frm_indx[j]], (Ny, Nx)), cmap='bwr')\n",
    "    ax[j][1].get_xaxis().set_visible(False)\n",
    "    ax[j][1].get_yaxis().set_visible(False)\n",
    "    ax[j][1].set_aspect('equal')\n",
    "\n",
    "    if j == 0:\n",
    "        ax[j][0].set_title('Original', fontsize=30)\n",
    "        ax[j][1].set_title('POD Approximation', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed748da",
   "metadata": {},
   "source": [
    "### DMD (Dynamic Modes Decomposition) - developed 2010s, some work done at UW:\n",
    "\n",
    "Look back to the POD, the POD modes are smooth & structured, the POD coefficients are rough & chaotic. This is because POD only sees spatial structure & ignores temporal dynamics. DMD attempts to address this issue by modeling the dynamics of the data:\n",
    "\n",
    "$$d_{k+1}=Ad_k$$\n",
    "$$f(x,t_{k+1}){\\leftarrow}A\\leftarrow f(x,t_k)$$\n",
    "\n",
    "Lets reconstruct our data as follows, one keeping the T-1 data from 0 to T-2, the other keeping the T-1 data from 1 to T-1.\n",
    "\n",
    "$$D_1:=\\begin{bmatrix} \n",
    "\\\\\n",
    "\td_0 \\;\\; | \\;\\; d_1\\;\\; | \\;\\; ....\\;\\; |\\;\\; d_{T-2}\\\\\n",
    "    \\\\\n",
    "\t\\end{bmatrix}$$\n",
    "\n",
    "$$D_2:=\\begin{bmatrix} \n",
    "\\\\\n",
    "\td_1 \\;\\; | \\;\\; d_2\\;\\; | \\;\\; ....\\;\\; |\\;\\; d_{T-1}\\\\\n",
    "    \\\\\n",
    "\t\\end{bmatrix}$$\n",
    "    \n",
    "Then DMD seeks to find a matrix $A$ such that \n",
    "$$D_2 \\approx AD_1 $$\n",
    "\n",
    "This approx is then done in a best-fit \n",
    "\n",
    "\n",
    "$$A=\\min_{B\\in \\mathbb{R}^{N\\times N},\\text{ rank(B)}\\le r}||BD_1-D_2||_F^2$$\n",
    "\n",
    "So what does DMD actually do?\n",
    "\n",
    "\\begin{align}\n",
    "d_k&=Ad_{k-1}=A(Ad_{k-2})=...\\\\\n",
    "d_k&=A^{k-1}d_0=Q\\Lambda^{k-1}Q^{-1}d_0 \\\\\n",
    "&=Q\\Lambda^{k-1}b=\\sum_{j=1}^rq_{j}\\lambda_{j}^{k-1}b_j\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydmd import DMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create a DMD object\n",
    "dmd = DMD(svd_rank=10)  # a rank 10 approximation to the dynamic matrix A\n",
    "\n",
    "# simply pass centered data set of snapshots to the dmd class\n",
    "dmd.fit(centered_data)\n",
    "\n",
    "dmd.plot_eigs(show_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the DMD modes and the temporal dynamics as we did for POD\n",
    "\n",
    "fig, ax = plt.subplots(8, 4, figsize=(30, 18))\n",
    "\n",
    "for j in range(8):\n",
    "\n",
    "    ax[j][0].imshow(np.reshape(np.real(dmd.modes[:, j]), (Ny, Nx)), cmap='bwr')\n",
    "    ax[j][0].get_xaxis().set_visible(False)\n",
    "    ax[j][0].get_yaxis().set_visible(False)\n",
    "    ax[j][0].set_aspect('equal')\n",
    "\n",
    "    ax[j][1].plot(np.real(dmd.dynamics[j, :]))\n",
    "    ax[j][1].set_xlabel('Time step')\n",
    "\n",
    "    ax[j][2].imshow(np.reshape(np.imag(dmd.modes[:, j]), (Ny, Nx)), cmap='bwr')\n",
    "    ax[j][2].get_xaxis().set_visible(False)\n",
    "    ax[j][2].get_yaxis().set_visible(False)\n",
    "    ax[j][2].set_aspect('equal')\n",
    "\n",
    "    ax[j][3].plot(np.imag(dmd.dynamics[j, :]))\n",
    "    ax[j][3].set_xlabel('Time step')\n",
    "\n",
    "    if j == 0:\n",
    "        ax[j][0].set_title('Real DMD Modes', fontsize=30)\n",
    "        ax[j][1].set_title('Real DMD Dynamics', fontsize=30)\n",
    "        ax[j][2].set_title('Imag DMD Modes', fontsize=30)\n",
    "        ax[j][3].set_title('Imag DMD Dynamics', fontsize=30)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1b773",
   "metadata": {},
   "source": [
    " ## Lecture 12\n",
    " ### Introduction to Machine Learning\n",
    " \n",
    " Book: The Elements of Statistical Learning by Hastie\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4805bb4",
   "metadata": {},
   "source": [
    "\\textbf{Supervised Learning}: With labels.\n",
    "Predict/classify data given a training dataset. Eg: REgression, classification, function approximation, etc.\n",
    "\n",
    "\\textbf{Un-supervised Learning}:\n",
    "No labels.\n",
    "Find meaningful structure in dataset. Eg: Clustering, dimensionality reduction, Generation modeling.\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "The function model assumes there exists a function\n",
    "$\\displaystyle f^+:X\\rightarrow y$ so that $y_i=f^+(x_j)+\\epsilon_j$, where $\\epsilon_j$ are some noise that may be in the output or our observation of the $f^+(x_j)$\n",
    "\n",
    "By far the most common assumption is Gaussian noise \n",
    "\n",
    "$$\\epsilon_j \\approx \\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "This implies $\\displaystyle y_i|x_j \\approx \\mathcal{N}(f^+(x_j),\\sigma^2)$ \n",
    "\n",
    "$$\\Pi(y_j|x_j)\\propto exp(-\\frac{1}{2\\sigma^2}|f^+(x_j)-y_j|^2)$$, $\\Pi$ is the PDF of $y$ for fixed $x_j$\n",
    "\n",
    "For Euclidern norm, this is called a maximum likelihood estimate (MLE):\n",
    "\n",
    "$$f_{MLE}=arg\\min_f \\frac{1}{2\\sigma^2}||f(X)-Y||^2$$\n",
    "\n",
    "At this moment, it is useless without a model, since there are many solutions. One of the most simple model is \\textbf{ linear regression}.\n",
    "\n",
    "$$f_{MLE}\\equiv\\beta_{MLE}=argmin \\frac{1}{2\\sigma^2}||A\\beta-Y||^2$$, where A=$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "1& & x_0^T \\\\\n",
    ".& & . \\\\\n",
    ".& & . \\\\\n",
    "1& & x_{N-1}^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "$.\n",
    "\n",
    "Therefore, MLE is nothing but a least square solution to the problem. Typically, the system is over-determined. Solution is given by solving the normal equations,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta}(\\frac{1}{2\\sigma^2}(A\\beta-y)^T(A\\beta-y))=\\frac{1}{\\sigma^2}A^T(A\\beta-y)=0$$\n",
    "\n",
    "\\begin{align}\\implies A^T(A\\beta-y)&=0\\\\\n",
    "\\beta&=(A^TA)^{-1}A^Ty\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548eaa8",
   "metadata": {},
   "source": [
    "## Lecture 13\n",
    "### Evaluating SL models\n",
    "\n",
    "\\textbf{ Regularization/penalization/shrinkage}:\n",
    "we consider $\\displaystyle \\hat{\\beta}=argmin \\frac{1}{2\\sigma^2}||A\\beta-y||^2+\\frac{\\lambda}{2}||\\beta||_p^p$\n",
    "\n",
    "$\\lambda\\ge 0$ is called the regularization/penalty parameter & $p \\ge 1$ denotes the choice. $p=2 $ for Ridge regression. \n",
    "\n",
    "$$\\beta=(\\frac{1}{\\sigma^2}A^TA+\\lambda I)^{-1}A^Ty$$\n",
    "\n",
    "So doing SVD of A,\n",
    "\n",
    "$$\\frac{1}{\\sigma^2}A^TA+\\lambda I=V(\\frac{1}{\\sigma^2}\\Sigma^2+ \\lambda I)V^T$$, the diagonals are non-negative, eliminating zeros so A matrix can be invertible.\n",
    "\n",
    "Again, the choice of $\\lambda$ is important for stability and accuracy.\n",
    "\n",
    "### Training & Testing Errors\n",
    "\n",
    "\\textbf{Training mean squared error (MSE)}\n",
    "\n",
    "${X,Y}$ - training set, used to find $\\hat{f}(\\equiv \\hat{\\beta})$\n",
    "\n",
    "${X',Y'}$ - testing set, used for validation.\n",
    "\n",
    "Analyzing the MSE doesn't mean which model is always better. It is still important for choice of $\\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf160c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67fa03bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e988083",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42511e17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58f0d30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154f0b1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c65757a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d51cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "297232fd",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "author": "mies",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
