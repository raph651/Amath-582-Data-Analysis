{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHaBz7jZVtfP"
   },
   "source": [
    "Companion notebook for Lecture 27. Demoing MLP (FNN) classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gyPWtY_V0Sl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.datasets import load_diabetes, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgUMK_NVV_d-",
    "outputId": "43261b08-2f59-4080-b5e4-c04ec525ac3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "X_b:  [[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "Y_b:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# load bread cancer data set\n",
    "\n",
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "feature_names = load_breast_cancer()['feature_names']\n",
    "\n",
    "print('features: ', feature_names)\n",
    "print('X_b: ', X)\n",
    "print('Y_b: ', Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0ycvs1CU_pC"
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.33, random_state= 42)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "X_train_s = X_scaler.transform(X_train)\n",
    "X_test_s = X_scaler.transform(X_test)\n",
    "\n",
    "# I also like to shift the output so we have +1, -1 for classes\n",
    "Y_train_s = 2*Y_train - 1\n",
    "Y_test_s = 2*Y_test - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idq3x6g6VEpa"
   },
   "outputs": [],
   "source": [
    "# fit Lasso \n",
    "bc_lassoCV = LassoCV( alphas = np.linspace(1e-6, 0.05, 40), cv= 10, max_iter = 50000 )\n",
    "\n",
    "bc_lassoCV.fit(X_train_s, Y_train_s)\n",
    "\n",
    "# fit Ridge \n",
    "bc_ridgeCV = RidgeCV( alphas = np.linspace(1e-6, 0.05, 40), cv= 10)\n",
    "\n",
    "bc_ridgeCV.fit(X_train_s, Y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mt_SohNYVbOj",
    "outputId": "880fa0e8-bcd1-4e21-f024-847db9c808ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge train MSE: 0.12598425196850394\n",
      "Lasso train MSE: 0.10498687664041995\n",
      "Ridge test MSE: 0.23404255319148937\n",
      "Lasso test MSE: 0.1702127659574468\n"
     ]
    }
   ],
   "source": [
    "# compute MSEs \n",
    "ridge_train_MSE = mean_squared_error( Y_train_s, np.sign(bc_ridgeCV.predict(X_train_s)) )\n",
    "ridge_test_MSE = mean_squared_error( Y_test_s, np.sign(bc_ridgeCV.predict(X_test_s)) )\n",
    "\n",
    "\n",
    "lasso_train_MSE = mean_squared_error( Y_train_s, np.sign(bc_lassoCV.predict(X_train_s)) )\n",
    "lasso_test_MSE = mean_squared_error( Y_test_s, np.sign(bc_lassoCV.predict(X_test_s)) )\n",
    "\n",
    "print('Ridge train MSE:', ridge_train_MSE)\n",
    "print('Lasso train MSE:', lasso_train_MSE)\n",
    "print('Ridge test MSE:', ridge_test_MSE)\n",
    "print('Lasso test MSE:', lasso_test_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyY1Z03tW3GW"
   },
   "outputs": [],
   "source": [
    "# fit MLP models \n",
    "\n",
    "lmbd = 1e-3\n",
    "rng = 10 #42, 10, 13 \n",
    "actf =  'relu' #'relu', 'tanh'\n",
    "\n",
    "bc_MLP_1 = MLPClassifier( hidden_layer_sizes= (5, 5, 5), activation = actf, alpha = lmbd, verbose= True, random_state= rng )\n",
    "bc_MLP_2 = MLPClassifier( hidden_layer_sizes= (15, 15, 15), activation = actf, alpha = lmbd, verbose= True, random_state = rng )\n",
    "bc_MLP_3 = MLPClassifier( hidden_layer_sizes= (30, 30, 30), activation = actf, alpha = lmbd, verbose= True, random_state = rng )\n",
    "bc_MLP_4 = MLPClassifier( hidden_layer_sizes= (40, 40, 40), activation = actf, alpha = lmbd, verbose= True, random_state = rng )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kS7Lc-3hX8Kz",
    "outputId": "8cd5d0af-1eae-4775-964d-d81463c2a26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Iteration 1, loss = 1.15034338\n",
      "Iteration 2, loss = 1.12599923\n",
      "Iteration 3, loss = 1.10190364\n",
      "Iteration 4, loss = 1.07852122\n",
      "Iteration 5, loss = 1.05593219\n",
      "Iteration 6, loss = 1.03422622\n",
      "Iteration 7, loss = 1.01266376\n",
      "Iteration 8, loss = 0.99187692\n",
      "Iteration 9, loss = 0.97166798\n",
      "Iteration 10, loss = 0.95227785\n",
      "Iteration 11, loss = 0.93286051\n",
      "Iteration 12, loss = 0.91433147\n",
      "Iteration 13, loss = 0.89592748\n",
      "Iteration 14, loss = 0.87842520\n",
      "Iteration 15, loss = 0.86051520\n",
      "Iteration 16, loss = 0.84333591\n",
      "Iteration 17, loss = 0.82612073\n",
      "Iteration 18, loss = 0.80966595\n",
      "Iteration 19, loss = 0.79307935\n",
      "Iteration 20, loss = 0.77675554\n",
      "Iteration 21, loss = 0.76052199\n",
      "Iteration 22, loss = 0.74517837\n",
      "Iteration 23, loss = 0.72950695\n",
      "Iteration 24, loss = 0.71364715\n",
      "Iteration 25, loss = 0.69806881\n",
      "Iteration 26, loss = 0.68307957\n",
      "Iteration 27, loss = 0.66808311\n",
      "Iteration 28, loss = 0.65286084\n",
      "Iteration 29, loss = 0.63843119\n",
      "Iteration 30, loss = 0.62351140\n",
      "Iteration 31, loss = 0.60917506\n",
      "Iteration 32, loss = 0.59480421\n",
      "Iteration 33, loss = 0.58080657\n",
      "Iteration 34, loss = 0.56708262\n",
      "Iteration 35, loss = 0.55363142\n",
      "Iteration 36, loss = 0.54015883\n",
      "Iteration 37, loss = 0.52717560\n",
      "Iteration 38, loss = 0.51414826\n",
      "Iteration 39, loss = 0.50170597\n",
      "Iteration 40, loss = 0.48927149\n",
      "Iteration 41, loss = 0.47727578\n",
      "Iteration 42, loss = 0.46541920\n",
      "Iteration 43, loss = 0.45404615\n",
      "Iteration 44, loss = 0.44296874\n",
      "Iteration 45, loss = 0.43209591\n",
      "Iteration 46, loss = 0.42152340\n",
      "Iteration 47, loss = 0.41109540\n",
      "Iteration 48, loss = 0.40125992\n",
      "Iteration 49, loss = 0.39141581\n",
      "Iteration 50, loss = 0.38187350\n",
      "Iteration 51, loss = 0.37284240\n",
      "Iteration 52, loss = 0.36393589\n",
      "Iteration 53, loss = 0.35534722\n",
      "Iteration 54, loss = 0.34694562\n",
      "Iteration 55, loss = 0.33881087\n",
      "Iteration 56, loss = 0.33089661\n",
      "Iteration 57, loss = 0.32313870\n",
      "Iteration 58, loss = 0.31565540\n",
      "Iteration 59, loss = 0.30850534\n",
      "Iteration 60, loss = 0.30143691\n",
      "Iteration 61, loss = 0.29455402\n",
      "Iteration 62, loss = 0.28794974\n",
      "Iteration 63, loss = 0.28143308\n",
      "Iteration 64, loss = 0.27500044\n",
      "Iteration 65, loss = 0.26909215\n",
      "Iteration 66, loss = 0.26304214\n",
      "Iteration 67, loss = 0.25751362\n",
      "Iteration 68, loss = 0.25205153\n",
      "Iteration 69, loss = 0.24683401\n",
      "Iteration 70, loss = 0.24190243\n",
      "Iteration 71, loss = 0.23719858\n",
      "Iteration 72, loss = 0.23245469\n",
      "Iteration 73, loss = 0.22822922\n",
      "Iteration 74, loss = 0.22390596\n",
      "Iteration 75, loss = 0.21977830\n",
      "Iteration 76, loss = 0.21578768\n",
      "Iteration 77, loss = 0.21202914\n",
      "Iteration 78, loss = 0.20836596\n",
      "Iteration 79, loss = 0.20487541\n",
      "Iteration 80, loss = 0.20143517\n",
      "Iteration 81, loss = 0.19814380\n",
      "Iteration 82, loss = 0.19493864\n",
      "Iteration 83, loss = 0.19191529\n",
      "Iteration 84, loss = 0.18883600\n",
      "Iteration 85, loss = 0.18596094\n",
      "Iteration 86, loss = 0.18320154\n",
      "Iteration 87, loss = 0.18041722\n",
      "Iteration 88, loss = 0.17779002\n",
      "Iteration 89, loss = 0.17523424\n",
      "Iteration 90, loss = 0.17285592\n",
      "Iteration 91, loss = 0.17036826\n",
      "Iteration 92, loss = 0.16801202\n",
      "Iteration 93, loss = 0.16576912\n",
      "Iteration 94, loss = 0.16359102\n",
      "Iteration 95, loss = 0.16139497\n",
      "Iteration 96, loss = 0.15935562\n",
      "Iteration 97, loss = 0.15731293\n",
      "Iteration 98, loss = 0.15534649\n",
      "Iteration 99, loss = 0.15341392\n",
      "Iteration 100, loss = 0.15151677\n",
      "Iteration 101, loss = 0.14975580\n",
      "Iteration 102, loss = 0.14796952\n",
      "Iteration 103, loss = 0.14627803\n",
      "Iteration 104, loss = 0.14460368\n",
      "Iteration 105, loss = 0.14292716\n",
      "Iteration 106, loss = 0.14133503\n",
      "Iteration 107, loss = 0.13975962\n",
      "Iteration 108, loss = 0.13825966\n",
      "Iteration 109, loss = 0.13681079\n",
      "Iteration 110, loss = 0.13542568\n",
      "Iteration 111, loss = 0.13393976\n",
      "Iteration 112, loss = 0.13259293\n",
      "Iteration 113, loss = 0.13126598\n",
      "Iteration 114, loss = 0.12998148\n",
      "Iteration 115, loss = 0.12870484\n",
      "Iteration 116, loss = 0.12744482\n",
      "Iteration 117, loss = 0.12629120\n",
      "Iteration 118, loss = 0.12504940\n",
      "Iteration 119, loss = 0.12395595\n",
      "Iteration 120, loss = 0.12285828\n",
      "Iteration 121, loss = 0.12174471\n",
      "Iteration 122, loss = 0.12067606\n",
      "Iteration 123, loss = 0.11963061\n",
      "Iteration 124, loss = 0.11863142\n",
      "Iteration 125, loss = 0.11761942\n",
      "Iteration 126, loss = 0.11663238\n",
      "Iteration 127, loss = 0.11570000\n",
      "Iteration 128, loss = 0.11475935\n",
      "Iteration 129, loss = 0.11385867\n",
      "Iteration 130, loss = 0.11295103\n",
      "Iteration 131, loss = 0.11210528\n",
      "Iteration 132, loss = 0.11124324\n",
      "Iteration 133, loss = 0.11038111\n",
      "Iteration 134, loss = 0.10958683\n",
      "Iteration 135, loss = 0.10876956\n",
      "Iteration 136, loss = 0.10797906\n",
      "Iteration 137, loss = 0.10719828\n",
      "Iteration 138, loss = 0.10644649\n",
      "Iteration 139, loss = 0.10567314\n",
      "Iteration 140, loss = 0.10496866\n",
      "Iteration 141, loss = 0.10422623\n",
      "Iteration 142, loss = 0.10352789\n",
      "Iteration 143, loss = 0.10284805\n",
      "Iteration 144, loss = 0.10216662\n",
      "Iteration 145, loss = 0.10149505\n",
      "Iteration 146, loss = 0.10082889\n",
      "Iteration 147, loss = 0.10018386\n",
      "Iteration 148, loss = 0.09952523\n",
      "Iteration 149, loss = 0.09891392\n",
      "Iteration 150, loss = 0.09828628\n",
      "Iteration 151, loss = 0.09769584\n",
      "Iteration 152, loss = 0.09710015\n",
      "Iteration 153, loss = 0.09650270\n",
      "Iteration 154, loss = 0.09592131\n",
      "Iteration 155, loss = 0.09536261\n",
      "Iteration 156, loss = 0.09476516\n",
      "Iteration 157, loss = 0.09423945\n",
      "Iteration 158, loss = 0.09369729\n",
      "Iteration 159, loss = 0.09314442\n",
      "Iteration 160, loss = 0.09263376\n",
      "Iteration 161, loss = 0.09210965\n",
      "Iteration 162, loss = 0.09159560\n",
      "Iteration 163, loss = 0.09109655\n",
      "Iteration 164, loss = 0.09060090\n",
      "Iteration 165, loss = 0.09011536\n",
      "Iteration 166, loss = 0.08960648\n",
      "Iteration 167, loss = 0.08913405\n",
      "Iteration 168, loss = 0.08865895\n",
      "Iteration 169, loss = 0.08821470\n",
      "Iteration 170, loss = 0.08772775\n",
      "Iteration 171, loss = 0.08726099\n",
      "Iteration 172, loss = 0.08683633\n",
      "Iteration 173, loss = 0.08640754\n",
      "Iteration 174, loss = 0.08595395\n",
      "Iteration 175, loss = 0.08552121\n",
      "Iteration 176, loss = 0.08509920\n",
      "Iteration 177, loss = 0.08468607\n",
      "Iteration 178, loss = 0.08426349\n",
      "Iteration 179, loss = 0.08383826\n",
      "Iteration 180, loss = 0.08344212\n",
      "Iteration 181, loss = 0.08305759\n",
      "Iteration 182, loss = 0.08264840\n",
      "Iteration 183, loss = 0.08225334\n",
      "Iteration 184, loss = 0.08186492\n",
      "Iteration 185, loss = 0.08150662\n",
      "Iteration 186, loss = 0.08111241\n",
      "Iteration 187, loss = 0.08073456\n",
      "Iteration 188, loss = 0.08035889\n",
      "Iteration 189, loss = 0.08001615\n",
      "Iteration 190, loss = 0.07962439\n",
      "Iteration 191, loss = 0.07928075\n",
      "Iteration 192, loss = 0.07895844\n",
      "Iteration 193, loss = 0.07857116\n",
      "Iteration 194, loss = 0.07822390\n",
      "Iteration 195, loss = 0.07787067\n",
      "Iteration 196, loss = 0.07756128\n",
      "Iteration 197, loss = 0.07720350\n",
      "Iteration 198, loss = 0.07689141\n",
      "Iteration 199, loss = 0.07655485\n",
      "Iteration 200, loss = 0.07622278\n",
      "Model 2:\n",
      "Iteration 1, loss = 0.65885043\n",
      "Iteration 2, loss = 0.61954448\n",
      "Iteration 3, loss = 0.58257300\n",
      "Iteration 4, loss = 0.54887608\n",
      "Iteration 5, loss = 0.51692527\n",
      "Iteration 6, loss = 0.48874439\n",
      "Iteration 7, loss = 0.46227418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.43716066\n",
      "Iteration 9, loss = 0.41444938\n",
      "Iteration 10, loss = 0.39345180\n",
      "Iteration 11, loss = 0.37377429\n",
      "Iteration 12, loss = 0.35546367\n",
      "Iteration 13, loss = 0.33888028\n",
      "Iteration 14, loss = 0.32260723\n",
      "Iteration 15, loss = 0.30778943\n",
      "Iteration 16, loss = 0.29403632\n",
      "Iteration 17, loss = 0.28114035\n",
      "Iteration 18, loss = 0.26890527\n",
      "Iteration 19, loss = 0.25765574\n",
      "Iteration 20, loss = 0.24710324\n",
      "Iteration 21, loss = 0.23699072\n",
      "Iteration 22, loss = 0.22758396\n",
      "Iteration 23, loss = 0.21887016\n",
      "Iteration 24, loss = 0.21069811\n",
      "Iteration 25, loss = 0.20308573\n",
      "Iteration 26, loss = 0.19576021\n",
      "Iteration 27, loss = 0.18900191\n",
      "Iteration 28, loss = 0.18270140\n",
      "Iteration 29, loss = 0.17678372\n",
      "Iteration 30, loss = 0.17115609\n",
      "Iteration 31, loss = 0.16600163\n",
      "Iteration 32, loss = 0.16097000\n",
      "Iteration 33, loss = 0.15636838\n",
      "Iteration 34, loss = 0.15185202\n",
      "Iteration 35, loss = 0.14778115\n",
      "Iteration 36, loss = 0.14376937\n",
      "Iteration 37, loss = 0.14022890\n",
      "Iteration 38, loss = 0.13690436\n",
      "Iteration 39, loss = 0.13337849\n",
      "Iteration 40, loss = 0.13020264\n",
      "Iteration 41, loss = 0.12719689\n",
      "Iteration 42, loss = 0.12453739\n",
      "Iteration 43, loss = 0.12170133\n",
      "Iteration 44, loss = 0.11924488\n",
      "Iteration 45, loss = 0.11668369\n",
      "Iteration 46, loss = 0.11435172\n",
      "Iteration 47, loss = 0.11223697\n",
      "Iteration 48, loss = 0.11002473\n",
      "Iteration 49, loss = 0.10786793\n",
      "Iteration 50, loss = 0.10601818\n",
      "Iteration 51, loss = 0.10417782\n",
      "Iteration 52, loss = 0.10226642\n",
      "Iteration 53, loss = 0.10058938\n",
      "Iteration 54, loss = 0.09891241\n",
      "Iteration 55, loss = 0.09724385\n",
      "Iteration 56, loss = 0.09574146\n",
      "Iteration 57, loss = 0.09423060\n",
      "Iteration 58, loss = 0.09275998\n",
      "Iteration 59, loss = 0.09143064\n",
      "Iteration 60, loss = 0.09001524\n",
      "Iteration 61, loss = 0.08871532\n",
      "Iteration 62, loss = 0.08747681\n",
      "Iteration 63, loss = 0.08621914\n",
      "Iteration 64, loss = 0.08507637\n",
      "Iteration 65, loss = 0.08393564\n",
      "Iteration 66, loss = 0.08281536\n",
      "Iteration 67, loss = 0.08179084\n",
      "Iteration 68, loss = 0.08073622\n",
      "Iteration 69, loss = 0.07972038\n",
      "Iteration 70, loss = 0.07877168\n",
      "Iteration 71, loss = 0.07784027\n",
      "Iteration 72, loss = 0.07686236\n",
      "Iteration 73, loss = 0.07598605\n",
      "Iteration 74, loss = 0.07514511\n",
      "Iteration 75, loss = 0.07433139\n",
      "Iteration 76, loss = 0.07350642\n",
      "Iteration 77, loss = 0.07269217\n",
      "Iteration 78, loss = 0.07188790\n",
      "Iteration 79, loss = 0.07111628\n",
      "Iteration 80, loss = 0.07039257\n",
      "Iteration 81, loss = 0.06967325\n",
      "Iteration 82, loss = 0.06897862\n",
      "Iteration 83, loss = 0.06832080\n",
      "Iteration 84, loss = 0.06763586\n",
      "Iteration 85, loss = 0.06692058\n",
      "Iteration 86, loss = 0.06630597\n",
      "Iteration 87, loss = 0.06566956\n",
      "Iteration 88, loss = 0.06505854\n",
      "Iteration 89, loss = 0.06452460\n",
      "Iteration 90, loss = 0.06389895\n",
      "Iteration 91, loss = 0.06330487\n",
      "Iteration 92, loss = 0.06271738\n",
      "Iteration 93, loss = 0.06214206\n",
      "Iteration 94, loss = 0.06163337\n",
      "Iteration 95, loss = 0.06112674\n",
      "Iteration 96, loss = 0.06054225\n",
      "Iteration 97, loss = 0.06008418\n",
      "Iteration 98, loss = 0.05950604\n",
      "Iteration 99, loss = 0.05900709\n",
      "Iteration 100, loss = 0.05853880\n",
      "Iteration 101, loss = 0.05807222\n",
      "Iteration 102, loss = 0.05762646\n",
      "Iteration 103, loss = 0.05710690\n",
      "Iteration 104, loss = 0.05664194\n",
      "Iteration 105, loss = 0.05621216\n",
      "Iteration 106, loss = 0.05575651\n",
      "Iteration 107, loss = 0.05529090\n",
      "Iteration 108, loss = 0.05486417\n",
      "Iteration 109, loss = 0.05443649\n",
      "Iteration 110, loss = 0.05398628\n",
      "Iteration 111, loss = 0.05364357\n",
      "Iteration 112, loss = 0.05317767\n",
      "Iteration 113, loss = 0.05283718\n",
      "Iteration 114, loss = 0.05237566\n",
      "Iteration 115, loss = 0.05199369\n",
      "Iteration 116, loss = 0.05160791\n",
      "Iteration 117, loss = 0.05119755\n",
      "Iteration 118, loss = 0.05082385\n",
      "Iteration 119, loss = 0.05048537\n",
      "Iteration 120, loss = 0.05009266\n",
      "Iteration 121, loss = 0.04968088\n",
      "Iteration 122, loss = 0.04936243\n",
      "Iteration 123, loss = 0.04897811\n",
      "Iteration 124, loss = 0.04867569\n",
      "Iteration 125, loss = 0.04826892\n",
      "Iteration 126, loss = 0.04797433\n",
      "Iteration 127, loss = 0.04762381\n",
      "Iteration 128, loss = 0.04730322\n",
      "Iteration 129, loss = 0.04696535\n",
      "Iteration 130, loss = 0.04665883\n",
      "Iteration 131, loss = 0.04626313\n",
      "Iteration 132, loss = 0.04591549\n",
      "Iteration 133, loss = 0.04568145\n",
      "Iteration 134, loss = 0.04528468\n",
      "Iteration 135, loss = 0.04499166\n",
      "Iteration 136, loss = 0.04467796\n",
      "Iteration 137, loss = 0.04435195\n",
      "Iteration 138, loss = 0.04404125\n",
      "Iteration 139, loss = 0.04379798\n",
      "Iteration 140, loss = 0.04346242\n",
      "Iteration 141, loss = 0.04320105\n",
      "Iteration 142, loss = 0.04288786\n",
      "Iteration 143, loss = 0.04262845\n",
      "Iteration 144, loss = 0.04230599\n",
      "Iteration 145, loss = 0.04207470\n",
      "Iteration 146, loss = 0.04180570\n",
      "Iteration 147, loss = 0.04145067\n",
      "Iteration 148, loss = 0.04124676\n",
      "Iteration 149, loss = 0.04086656\n",
      "Iteration 150, loss = 0.04063918\n",
      "Iteration 151, loss = 0.04037462\n",
      "Iteration 152, loss = 0.04010742\n",
      "Iteration 153, loss = 0.03985415\n",
      "Iteration 154, loss = 0.03960891\n",
      "Iteration 155, loss = 0.03932977\n",
      "Iteration 156, loss = 0.03906900\n",
      "Iteration 157, loss = 0.03883986\n",
      "Iteration 158, loss = 0.03857026\n",
      "Iteration 159, loss = 0.03836369\n",
      "Iteration 160, loss = 0.03806355\n",
      "Iteration 161, loss = 0.03782116\n",
      "Iteration 162, loss = 0.03762551\n",
      "Iteration 163, loss = 0.03740237\n",
      "Iteration 164, loss = 0.03715109\n",
      "Iteration 165, loss = 0.03688207\n",
      "Iteration 166, loss = 0.03665524\n",
      "Iteration 167, loss = 0.03644958\n",
      "Iteration 168, loss = 0.03623467\n",
      "Iteration 169, loss = 0.03603923\n",
      "Iteration 170, loss = 0.03578778\n",
      "Iteration 171, loss = 0.03555057\n",
      "Iteration 172, loss = 0.03532440\n",
      "Iteration 173, loss = 0.03510712\n",
      "Iteration 174, loss = 0.03496093\n",
      "Iteration 175, loss = 0.03470616\n",
      "Iteration 176, loss = 0.03447515\n",
      "Iteration 177, loss = 0.03427315\n",
      "Iteration 178, loss = 0.03409535\n",
      "Iteration 179, loss = 0.03392906\n",
      "Iteration 180, loss = 0.03374005\n",
      "Iteration 181, loss = 0.03349357\n",
      "Iteration 182, loss = 0.03334261\n",
      "Iteration 183, loss = 0.03312491\n",
      "Iteration 184, loss = 0.03291431\n",
      "Iteration 185, loss = 0.03274970\n",
      "Iteration 186, loss = 0.03255461\n",
      "Iteration 187, loss = 0.03237761\n",
      "Iteration 188, loss = 0.03224033\n",
      "Iteration 189, loss = 0.03205546\n",
      "Iteration 190, loss = 0.03186494\n",
      "Iteration 191, loss = 0.03169970\n",
      "Iteration 192, loss = 0.03149786\n",
      "Iteration 193, loss = 0.03133367\n",
      "Iteration 194, loss = 0.03118259\n",
      "Iteration 195, loss = 0.03099780\n",
      "Iteration 196, loss = 0.03089207\n",
      "Iteration 197, loss = 0.03068467\n",
      "Iteration 198, loss = 0.03052706\n",
      "Iteration 199, loss = 0.03038889\n",
      "Iteration 200, loss = 0.03021448\n",
      "Model 3:\n",
      "Iteration 1, loss = 0.93003741\n",
      "Iteration 2, loss = 0.83037054\n",
      "Iteration 3, loss = 0.74408229\n",
      "Iteration 4, loss = 0.66455832\n",
      "Iteration 5, loss = 0.59405244\n",
      "Iteration 6, loss = 0.53107317\n",
      "Iteration 7, loss = 0.47653351"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 8, loss = 0.42840358\n",
      "Iteration 9, loss = 0.38829072\n",
      "Iteration 10, loss = 0.35151866\n",
      "Iteration 11, loss = 0.31973371\n",
      "Iteration 12, loss = 0.29265466\n",
      "Iteration 13, loss = 0.26829652\n",
      "Iteration 14, loss = 0.24748001\n",
      "Iteration 15, loss = 0.22904586\n",
      "Iteration 16, loss = 0.21218322\n",
      "Iteration 17, loss = 0.19740521\n",
      "Iteration 18, loss = 0.18463161\n",
      "Iteration 19, loss = 0.17256893\n",
      "Iteration 20, loss = 0.16290315\n",
      "Iteration 21, loss = 0.15358260\n",
      "Iteration 22, loss = 0.14586219\n",
      "Iteration 23, loss = 0.13844143\n",
      "Iteration 24, loss = 0.13177889\n",
      "Iteration 25, loss = 0.12591145\n",
      "Iteration 26, loss = 0.12072640\n",
      "Iteration 27, loss = 0.11587756\n",
      "Iteration 28, loss = 0.11178105\n",
      "Iteration 29, loss = 0.10773557\n",
      "Iteration 30, loss = 0.10431319\n",
      "Iteration 31, loss = 0.10078258\n",
      "Iteration 32, loss = 0.09764282\n",
      "Iteration 33, loss = 0.09496979\n",
      "Iteration 34, loss = 0.09249034\n",
      "Iteration 35, loss = 0.09002914\n",
      "Iteration 36, loss = 0.08772399\n",
      "Iteration 37, loss = 0.08567705\n",
      "Iteration 38, loss = 0.08385018\n",
      "Iteration 39, loss = 0.08205030\n",
      "Iteration 40, loss = 0.08047497\n",
      "Iteration 41, loss = 0.07897460\n",
      "Iteration 42, loss = 0.07745034\n",
      "Iteration 43, loss = 0.07610168\n",
      "Iteration 44, loss = 0.07483747\n",
      "Iteration 45, loss = 0.07370471\n",
      "Iteration 46, loss = 0.07243524\n",
      "Iteration 47, loss = 0.07134309\n",
      "Iteration 48, loss = 0.07026579\n",
      "Iteration 49, loss = 0.06924326\n",
      "Iteration 50, loss = 0.06828186\n",
      "Iteration 51, loss = 0.06740743\n",
      "Iteration 52, loss = 0.06648965\n",
      "Iteration 53, loss = 0.06555651\n",
      "Iteration 54, loss = 0.06470553\n",
      "Iteration 55, loss = 0.06402517\n",
      "Iteration 56, loss = 0.06307902\n",
      "Iteration 57, loss = 0.06237694\n",
      "Iteration 58, loss = 0.06165227\n",
      "Iteration 59, loss = 0.06087267\n",
      "Iteration 60, loss = 0.06019370\n",
      "Iteration 61, loss = 0.05957077\n",
      "Iteration 62, loss = 0.05894253\n",
      "Iteration 63, loss = 0.05825089\n",
      "Iteration 64, loss = 0.05762128\n",
      "Iteration 65, loss = 0.05700866\n",
      "Iteration 66, loss = 0.05640304\n",
      "Iteration 67, loss = 0.05585153\n",
      "Iteration 68, loss = 0.05523960\n",
      "Iteration 69, loss = 0.05470100\n",
      "Iteration 70, loss = 0.05412720\n",
      "Iteration 71, loss = 0.05356690\n",
      "Iteration 72, loss = 0.05309206\n",
      "Iteration 73, loss = 0.05260383\n",
      "Iteration 74, loss = 0.05208228\n",
      "Iteration 75, loss = 0.05160166\n",
      "Iteration 76, loss = 0.05107281\n",
      "Iteration 77, loss = 0.05072802\n",
      "Iteration 78, loss = 0.05017413\n",
      "Iteration 79, loss = 0.04965924\n",
      "Iteration 80, loss = 0.04926205\n",
      "Iteration 81, loss = 0.04885743\n",
      "Iteration 82, loss = 0.04834662\n",
      "Iteration 83, loss = 0.04793195\n",
      "Iteration 84, loss = 0.04755904\n",
      "Iteration 85, loss = 0.04710003\n",
      "Iteration 86, loss = 0.04678894\n",
      "Iteration 87, loss = 0.04625943\n",
      "Iteration 88, loss = 0.04589776\n",
      "Iteration 89, loss = 0.04544895\n",
      "Iteration 90, loss = 0.04515355\n",
      "Iteration 91, loss = 0.04469431\n",
      "Iteration 92, loss = 0.04434863\n",
      "Iteration 93, loss = 0.04408552\n",
      "Iteration 94, loss = 0.04354983\n",
      "Iteration 95, loss = 0.04318383\n",
      "Iteration 96, loss = 0.04283373\n",
      "Iteration 97, loss = 0.04243843\n",
      "Iteration 98, loss = 0.04206588\n",
      "Iteration 99, loss = 0.04172727\n",
      "Iteration 100, loss = 0.04137499\n",
      "Iteration 101, loss = 0.04100753\n",
      "Iteration 102, loss = 0.04066565\n",
      "Iteration 103, loss = 0.04029671\n",
      "Iteration 104, loss = 0.03994004\n",
      "Iteration 105, loss = 0.03962965\n",
      "Iteration 106, loss = 0.03923683\n",
      "Iteration 107, loss = 0.03889800\n",
      "Iteration 108, loss = 0.03856592\n",
      "Iteration 109, loss = 0.03820543\n",
      "Iteration 110, loss = 0.03793581\n",
      "Iteration 111, loss = 0.03751703\n",
      "Iteration 112, loss = 0.03714001\n",
      "Iteration 113, loss = 0.03676210\n",
      "Iteration 114, loss = 0.03645395\n",
      "Iteration 115, loss = 0.03614826\n",
      "Iteration 116, loss = 0.03572065\n",
      "Iteration 117, loss = 0.03540354\n",
      "Iteration 118, loss = 0.03511212\n",
      "Iteration 119, loss = 0.03471936\n",
      "Iteration 120, loss = 0.03437758\n",
      "Iteration 121, loss = 0.03403957\n",
      "Iteration 122, loss = 0.03366018\n",
      "Iteration 123, loss = 0.03337238\n",
      "Iteration 124, loss = 0.03298804\n",
      "Iteration 125, loss = 0.03261585\n",
      "Iteration 126, loss = 0.03223417\n",
      "Iteration 127, loss = 0.03192550\n",
      "Iteration 128, loss = 0.03157302\n",
      "Iteration 129, loss = 0.03122926\n",
      "Iteration 130, loss = 0.03092225\n",
      "Iteration 131, loss = 0.03047093\n",
      "Iteration 132, loss = 0.03019034\n",
      "Iteration 133, loss = 0.02982826\n",
      "Iteration 134, loss = 0.02946425\n",
      "Iteration 135, loss = 0.02910752\n",
      "Iteration 136, loss = 0.02880799\n",
      "Iteration 137, loss = 0.02848205\n",
      "Iteration 138, loss = 0.02811697\n",
      "Iteration 139, loss = 0.02768419\n",
      "Iteration 140, loss = 0.02743761\n",
      "Iteration 141, loss = 0.02703143\n",
      "Iteration 142, loss = 0.02673764\n",
      "Iteration 143, loss = 0.02634233\n",
      "Iteration 144, loss = 0.02610112\n",
      "Iteration 145, loss = 0.02568151\n",
      "Iteration 146, loss = 0.02546813\n",
      "Iteration 147, loss = 0.02503692\n",
      "Iteration 148, loss = 0.02477270\n",
      "Iteration 149, loss = 0.02445901\n",
      "Iteration 150, loss = 0.02409647\n",
      "Iteration 151, loss = 0.02383022\n",
      "Iteration 152, loss = 0.02348569\n",
      "Iteration 153, loss = 0.02314986\n",
      "Iteration 154, loss = 0.02281263\n",
      "Iteration 155, loss = 0.02259479\n",
      "Iteration 156, loss = 0.02221971\n",
      "Iteration 157, loss = 0.02196799\n",
      "Iteration 158, loss = 0.02172615\n",
      "Iteration 159, loss = 0.02136464\n",
      "Iteration 160, loss = 0.02106248\n",
      "Iteration 161, loss = 0.02081142\n",
      "Iteration 162, loss = 0.02052825\n",
      "Iteration 163, loss = 0.02024091\n",
      "Iteration 164, loss = 0.01996510\n",
      "Iteration 165, loss = 0.01972986\n",
      "Iteration 166, loss = 0.01945136\n",
      "Iteration 167, loss = 0.01915182\n",
      "Iteration 168, loss = 0.01887120\n",
      "Iteration 169, loss = 0.01860326\n",
      "Iteration 170, loss = 0.01842907\n",
      "Iteration 171, loss = 0.01816515\n",
      "Iteration 172, loss = 0.01786989\n",
      "Iteration 173, loss = 0.01760240\n",
      "Iteration 174, loss = 0.01735501\n",
      "Iteration 175, loss = 0.01711471\n",
      "Iteration 176, loss = 0.01691717\n",
      "Iteration 177, loss = 0.01671545\n",
      "Iteration 178, loss = 0.01639690\n",
      "Iteration 179, loss = 0.01619824\n",
      "Iteration 180, loss = 0.01609173\n",
      "Iteration 181, loss = 0.01578097\n",
      "Iteration 182, loss = 0.01553488\n",
      "Iteration 183, loss = 0.01531821\n",
      "Iteration 184, loss = 0.01508708\n",
      "Iteration 185, loss = 0.01492608\n",
      "Iteration 186, loss = 0.01472825\n",
      "Iteration 187, loss = 0.01455734\n",
      "Iteration 188, loss = 0.01427178\n",
      "Iteration 189, loss = 0.01412529\n",
      "Iteration 190, loss = 0.01387515\n",
      "Iteration 191, loss = 0.01365618\n",
      "Iteration 192, loss = 0.01345735\n",
      "Iteration 193, loss = 0.01329947\n",
      "Iteration 194, loss = 0.01306775\n",
      "Iteration 195, loss = 0.01291876\n",
      "Iteration 196, loss = 0.01272366\n",
      "Iteration 197, loss = 0.01264044\n",
      "Iteration 198, loss = 0.01237554\n",
      "Iteration 199, loss = 0.01225582\n",
      "Iteration 200, loss = 0.01200354\n",
      "Model 4:\n",
      "Iteration 1, loss = 0.73434743\n",
      "Iteration 2, loss = 0.59668158\n",
      "Iteration 3, loss = 0.48851711\n",
      "Iteration 4, loss = 0.40641759\n",
      "Iteration 5, loss = 0.34395685\n",
      "Iteration 6, loss = 0.29720183\n",
      "Iteration 7, loss = 0.25916921\n",
      "Iteration 8, loss = 0.22983904\n",
      "Iteration 9, loss = 0.20584255\n",
      "Iteration 10, loss = 0.18549445\n",
      "Iteration 11, loss = 0.17004265\n",
      "Iteration 12, loss = 0.15674181\n",
      "Iteration 13, loss = 0.14480534\n",
      "Iteration 14, loss = 0.13484371\n",
      "Iteration 15, loss = 0.12740082\n",
      "Iteration 16, loss = 0.12022097\n",
      "Iteration 17, loss = 0.11403815\n",
      "Iteration 18, loss = 0.10849640\n",
      "Iteration 19, loss = 0.10379556\n",
      "Iteration 20, loss = 0.09990057\n",
      "Iteration 21, loss = 0.09591251\n",
      "Iteration 22, loss = 0.09257168\n",
      "Iteration 23, loss = 0.08949368\n",
      "Iteration 24, loss = 0.08684645\n",
      "Iteration 25, loss = 0.08437027\n",
      "Iteration 26, loss = 0.08199011\n",
      "Iteration 27, loss = 0.08000211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.07818734\n",
      "Iteration 29, loss = 0.07632043\n",
      "Iteration 30, loss = 0.07474834\n",
      "Iteration 31, loss = 0.07322129\n",
      "Iteration 32, loss = 0.07173424\n",
      "Iteration 33, loss = 0.07022448\n",
      "Iteration 34, loss = 0.06885063\n",
      "Iteration 35, loss = 0.06773280\n",
      "Iteration 36, loss = 0.06646934\n",
      "Iteration 37, loss = 0.06528772\n",
      "Iteration 38, loss = 0.06442722\n",
      "Iteration 39, loss = 0.06320497\n",
      "Iteration 40, loss = 0.06237217\n",
      "Iteration 41, loss = 0.06133970\n",
      "Iteration 42, loss = 0.06047640\n",
      "Iteration 43, loss = 0.05962643\n",
      "Iteration 44, loss = 0.05869853\n",
      "Iteration 45, loss = 0.05787552\n",
      "Iteration 46, loss = 0.05706745\n",
      "Iteration 47, loss = 0.05632179\n",
      "Iteration 48, loss = 0.05563183\n",
      "Iteration 49, loss = 0.05480680\n",
      "Iteration 50, loss = 0.05418150\n",
      "Iteration 51, loss = 0.05347464\n",
      "Iteration 52, loss = 0.05277377\n",
      "Iteration 53, loss = 0.05209492\n",
      "Iteration 54, loss = 0.05143932\n",
      "Iteration 55, loss = 0.05080926\n",
      "Iteration 56, loss = 0.05020777\n",
      "Iteration 57, loss = 0.04966978\n",
      "Iteration 58, loss = 0.04902069\n",
      "Iteration 59, loss = 0.04841515\n",
      "Iteration 60, loss = 0.04785475\n",
      "Iteration 61, loss = 0.04735978\n",
      "Iteration 62, loss = 0.04674754\n",
      "Iteration 63, loss = 0.04617959\n",
      "Iteration 64, loss = 0.04567640\n",
      "Iteration 65, loss = 0.04513588\n",
      "Iteration 66, loss = 0.04463827\n",
      "Iteration 67, loss = 0.04406159\n",
      "Iteration 68, loss = 0.04359959\n",
      "Iteration 69, loss = 0.04307301\n",
      "Iteration 70, loss = 0.04260622\n",
      "Iteration 71, loss = 0.04212371\n",
      "Iteration 72, loss = 0.04161570\n",
      "Iteration 73, loss = 0.04115773\n",
      "Iteration 74, loss = 0.04069973\n",
      "Iteration 75, loss = 0.04018950\n",
      "Iteration 76, loss = 0.03976523\n",
      "Iteration 77, loss = 0.03935146\n",
      "Iteration 78, loss = 0.03882146\n",
      "Iteration 79, loss = 0.03838675\n",
      "Iteration 80, loss = 0.03807373\n",
      "Iteration 81, loss = 0.03754087\n",
      "Iteration 82, loss = 0.03712723\n",
      "Iteration 83, loss = 0.03675529\n",
      "Iteration 84, loss = 0.03620959\n",
      "Iteration 85, loss = 0.03575434\n",
      "Iteration 86, loss = 0.03543407\n",
      "Iteration 87, loss = 0.03494852\n",
      "Iteration 88, loss = 0.03454147\n",
      "Iteration 89, loss = 0.03411556\n",
      "Iteration 90, loss = 0.03371076\n",
      "Iteration 91, loss = 0.03323529\n",
      "Iteration 92, loss = 0.03289680\n",
      "Iteration 93, loss = 0.03244199\n",
      "Iteration 94, loss = 0.03204764\n",
      "Iteration 95, loss = 0.03159422\n",
      "Iteration 96, loss = 0.03122349\n",
      "Iteration 97, loss = 0.03081227\n",
      "Iteration 98, loss = 0.03046471\n",
      "Iteration 99, loss = 0.03006607\n",
      "Iteration 100, loss = 0.02961797\n",
      "Iteration 101, loss = 0.02920901\n",
      "Iteration 102, loss = 0.02885441\n",
      "Iteration 103, loss = 0.02846651\n",
      "Iteration 104, loss = 0.02808194\n",
      "Iteration 105, loss = 0.02766251\n",
      "Iteration 106, loss = 0.02730515\n",
      "Iteration 107, loss = 0.02690172\n",
      "Iteration 108, loss = 0.02654214\n",
      "Iteration 109, loss = 0.02619385\n",
      "Iteration 110, loss = 0.02577702\n",
      "Iteration 111, loss = 0.02544004\n",
      "Iteration 112, loss = 0.02506122\n",
      "Iteration 113, loss = 0.02464664\n",
      "Iteration 114, loss = 0.02427210\n",
      "Iteration 115, loss = 0.02386493\n",
      "Iteration 116, loss = 0.02356353\n",
      "Iteration 117, loss = 0.02322582\n",
      "Iteration 118, loss = 0.02281597\n",
      "Iteration 119, loss = 0.02239791\n",
      "Iteration 120, loss = 0.02200717\n",
      "Iteration 121, loss = 0.02171060\n",
      "Iteration 122, loss = 0.02132254\n",
      "Iteration 123, loss = 0.02094281\n",
      "Iteration 124, loss = 0.02065128\n",
      "Iteration 125, loss = 0.02036402\n",
      "Iteration 126, loss = 0.01998375\n",
      "Iteration 127, loss = 0.01959598\n",
      "Iteration 128, loss = 0.01921706\n",
      "Iteration 129, loss = 0.01893015\n",
      "Iteration 130, loss = 0.01861528\n",
      "Iteration 131, loss = 0.01829486\n",
      "Iteration 132, loss = 0.01792771\n",
      "Iteration 133, loss = 0.01763598\n",
      "Iteration 134, loss = 0.01729698\n",
      "Iteration 135, loss = 0.01695479\n",
      "Iteration 136, loss = 0.01665039\n",
      "Iteration 137, loss = 0.01637726\n",
      "Iteration 138, loss = 0.01610516\n",
      "Iteration 139, loss = 0.01573917\n",
      "Iteration 140, loss = 0.01545188\n",
      "Iteration 141, loss = 0.01522579\n",
      "Iteration 142, loss = 0.01485035\n",
      "Iteration 143, loss = 0.01462169\n",
      "Iteration 144, loss = 0.01436069\n",
      "Iteration 145, loss = 0.01401847\n",
      "Iteration 146, loss = 0.01379664\n",
      "Iteration 147, loss = 0.01349321\n",
      "Iteration 148, loss = 0.01321537\n",
      "Iteration 149, loss = 0.01301766\n",
      "Iteration 150, loss = 0.01277277\n",
      "Iteration 151, loss = 0.01246777\n",
      "Iteration 152, loss = 0.01227418\n",
      "Iteration 153, loss = 0.01201545\n",
      "Iteration 154, loss = 0.01178568\n",
      "Iteration 155, loss = 0.01155945\n",
      "Iteration 156, loss = 0.01131050\n",
      "Iteration 157, loss = 0.01106442\n",
      "Iteration 158, loss = 0.01087480\n",
      "Iteration 159, loss = 0.01063587\n",
      "Iteration 160, loss = 0.01042178\n",
      "Iteration 161, loss = 0.01028816\n",
      "Iteration 162, loss = 0.01010900\n",
      "Iteration 163, loss = 0.00986808\n",
      "Iteration 164, loss = 0.00962916\n",
      "Iteration 165, loss = 0.00949470\n",
      "Iteration 166, loss = 0.00932487\n",
      "Iteration 167, loss = 0.00910951\n",
      "Iteration 168, loss = 0.00893198\n",
      "Iteration 169, loss = 0.00878214\n",
      "Iteration 170, loss = 0.00859043\n",
      "Iteration 171, loss = 0.00840253\n",
      "Iteration 172, loss = 0.00827721\n",
      "Iteration 173, loss = 0.00807603\n",
      "Iteration 174, loss = 0.00793630\n",
      "Iteration 175, loss = 0.00779377\n",
      "Iteration 176, loss = 0.00764815\n",
      "Iteration 177, loss = 0.00754518\n",
      "Iteration 178, loss = 0.00735144\n",
      "Iteration 179, loss = 0.00721256\n",
      "Iteration 180, loss = 0.00710773\n",
      "Iteration 181, loss = 0.00695159\n",
      "Iteration 182, loss = 0.00681514\n",
      "Iteration 183, loss = 0.00673079\n",
      "Iteration 184, loss = 0.00655329\n",
      "Iteration 185, loss = 0.00647682\n",
      "Iteration 186, loss = 0.00633536\n",
      "Iteration 187, loss = 0.00622004\n",
      "Iteration 188, loss = 0.00609467\n",
      "Iteration 189, loss = 0.00602484\n",
      "Iteration 190, loss = 0.00588329\n",
      "Iteration 191, loss = 0.00583976\n",
      "Iteration 192, loss = 0.00568681\n",
      "Iteration 193, loss = 0.00557300\n",
      "Iteration 194, loss = 0.00547690\n",
      "Iteration 195, loss = 0.00537866\n",
      "Iteration 196, loss = 0.00528140\n",
      "Iteration 197, loss = 0.00519658\n",
      "Iteration 198, loss = 0.00510520\n",
      "Iteration 199, loss = 0.00501705\n",
      "Iteration 200, loss = 0.00494580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, hidden_layer_sizes=(40, 40, 40),\n",
       "              random_state=10, verbose=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all models \n",
    "\n",
    "print('Model 1:')\n",
    "bc_MLP_1.fit(X_train_s, Y_train_s)\n",
    "print('Model 2:')\n",
    "bc_MLP_2.fit(X_train_s, Y_train_s)\n",
    "print('Model 3:')\n",
    "bc_MLP_3.fit(X_train_s, Y_train_s)\n",
    "print('Model 4:')\n",
    "bc_MLP_4.fit(X_train_s, Y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snJprFfnYxAs"
   },
   "outputs": [],
   "source": [
    "# predict and compute MSEs \n",
    "\n",
    "m1_train_MSE = mean_squared_error( Y_train_s, bc_MLP_1.predict(X_train_s) )\n",
    "m1_test_MSE = mean_squared_error( Y_test_s, bc_MLP_1.predict(X_test_s) )\n",
    "\n",
    "m2_train_MSE = mean_squared_error( Y_train_s, bc_MLP_2.predict(X_train_s) )\n",
    "m2_test_MSE = mean_squared_error( Y_test_s, bc_MLP_2.predict(X_test_s) )\n",
    "\n",
    "m3_train_MSE = mean_squared_error( Y_train_s, bc_MLP_3.predict(X_train_s) )\n",
    "m3_test_MSE = mean_squared_error( Y_test_s, bc_MLP_3.predict(X_test_s) )\n",
    "\n",
    "m4_train_MSE = mean_squared_error( Y_train_s, bc_MLP_4.predict(X_train_s) )\n",
    "m4_test_MSE = mean_squared_error( Y_test_s, bc_MLP_4.predict(X_test_s) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15r0zvNQZl4T",
    "outputId": "478c7333-3268-456a-abd5-3b018c0f4df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge train MSE: 0.12598425196850394\n",
      "Lasso train MSE: 0.10498687664041995\n",
      "Model 1 train MSE: 0.05249343832020997\n",
      "Model 2 train MSE: 0.02099737532808399\n",
      "Model 3 train MSE: 0.010498687664041995\n",
      "Model 4 train MSE: 0.0\n",
      "Ridge test MSE: 0.23404255319148937\n",
      "Lasso test MSE: 0.1702127659574468\n",
      "Model 1 test MSE: 0.06382978723404255\n",
      "Model 2 test MSE: 0.0851063829787234\n",
      "Model 3 test MSE: 0.10638297872340426\n",
      "Model 4 test MSE: 0.10638297872340426\n"
     ]
    }
   ],
   "source": [
    "# Print MSEs and compare to Ridge and Lasso \n",
    "\n",
    "print('Ridge train MSE:', ridge_train_MSE)\n",
    "print('Lasso train MSE:', lasso_train_MSE)\n",
    "print('Model 1 train MSE:', m1_train_MSE)\n",
    "print('Model 2 train MSE:', m2_train_MSE)\n",
    "print('Model 3 train MSE:', m3_train_MSE)\n",
    "print('Model 4 train MSE:', m4_train_MSE)\n",
    "\n",
    "print('Ridge test MSE:', ridge_test_MSE)\n",
    "print('Lasso test MSE:', lasso_test_MSE)\n",
    "print('Model 1 test MSE:', m1_test_MSE)\n",
    "print('Model 2 test MSE:', m2_test_MSE)\n",
    "print('Model 3 test MSE:', m3_test_MSE)\n",
    "print('Model 4 test MSE:', m4_test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK_kju7waB4t"
   },
   "outputs": [],
   "source": [
    "# deep networks \n",
    "\n",
    "bc_MLP_5 = MLPClassifier( hidden_layer_sizes= tuple(40*np.ones(10, dtype = int)), activation = actf, alpha = lmbd, verbose= True, random_state = rng )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DPhwC0wc93o",
    "outputId": "2f2f08f7-51be-476f-f756-c8c77077cdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deep:\n",
      "Iteration 1, loss = 0.60045672\n",
      "Iteration 2, loss = 0.44297765\n",
      "Iteration 3, loss = 0.33393310\n",
      "Iteration 4, loss = 0.25452213\n",
      "Iteration 5, loss = 0.20151712\n",
      "Iteration 6, loss = 0.16159676\n",
      "Iteration 7, loss = 0.13523633\n",
      "Iteration 8, loss = 0.11578063\n",
      "Iteration 9, loss = 0.10433439\n",
      "Iteration 10, loss = 0.09167084\n",
      "Iteration 11, loss = 0.08477216\n",
      "Iteration 12, loss = 0.07674046\n",
      "Iteration 13, loss = 0.07419161\n",
      "Iteration 14, loss = 0.07218267\n",
      "Iteration 15, loss = 0.06803737\n",
      "Iteration 16, loss = 0.06469357\n",
      "Iteration 17, loss = 0.06150326\n",
      "Iteration 18, loss = 0.05730405\n",
      "Iteration 19, loss = 0.05451799\n",
      "Iteration 20, loss = 0.05086591\n",
      "Iteration 21, loss = 0.04909471\n",
      "Iteration 22, loss = 0.04705828\n",
      "Iteration 23, loss = 0.04623676\n",
      "Iteration 24, loss = 0.04446524\n",
      "Iteration 25, loss = 0.04275337\n",
      "Iteration 26, loss = 0.04067046\n",
      "Iteration 27, loss = 0.03997573\n",
      "Iteration 28, loss = 0.03749039\n",
      "Iteration 29, loss = 0.03635926\n",
      "Iteration 30, loss = 0.03540536\n",
      "Iteration 31, loss = 0.03451454\n",
      "Iteration 32, loss = 0.03301846\n",
      "Iteration 33, loss = 0.03128149\n",
      "Iteration 34, loss = 0.03004405\n",
      "Iteration 35, loss = 0.02906536\n",
      "Iteration 36, loss = 0.02737037\n",
      "Iteration 37, loss = 0.02620716\n",
      "Iteration 38, loss = 0.02524735\n",
      "Iteration 39, loss = 0.02409532\n",
      "Iteration 40, loss = 0.02373227\n",
      "Iteration 41, loss = 0.02284159\n",
      "Iteration 42, loss = 0.02220943\n",
      "Iteration 43, loss = 0.02180287\n",
      "Iteration 44, loss = 0.02153028\n",
      "Iteration 45, loss = 0.02108634\n",
      "Iteration 46, loss = 0.02088671\n",
      "Iteration 47, loss = 0.02082775\n",
      "Iteration 48, loss = 0.02066881\n",
      "Iteration 49, loss = 0.02041249\n",
      "Iteration 50, loss = 0.02028375\n",
      "Iteration 51, loss = 0.02020418\n",
      "Iteration 52, loss = 0.02015503\n",
      "Iteration 53, loss = 0.02005898\n",
      "Iteration 54, loss = 0.02002157\n",
      "Iteration 55, loss = 0.01996338\n",
      "Iteration 56, loss = 0.01992884\n",
      "Iteration 57, loss = 0.01989729\n",
      "Iteration 58, loss = 0.01984523\n",
      "Iteration 59, loss = 0.01980512\n",
      "Iteration 60, loss = 0.01977333\n",
      "Iteration 61, loss = 0.01975169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "print('Model deep:')\n",
    "bc_MLP_5.fit(X_train_s, Y_train_s)\n",
    "\n",
    "\n",
    "m5_train_MSE = mean_squared_error( Y_train_s, bc_MLP_5.predict(X_train_s) )\n",
    "m5_test_MSE = mean_squared_error( Y_test_s, bc_MLP_5.predict(X_test_s) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFgWWjfMdMgr",
    "outputId": "e90af579-d9ae-4e3b-9f12-5283b8e8d855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deep train MSE: 0.010498687664041995\n",
      "Model deep test MSE: 0.0851063829787234\n"
     ]
    }
   ],
   "source": [
    "print('Model deep train MSE:', m5_train_MSE)\n",
    "print('Model deep test MSE:', m5_test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHt0FW31dXnz"
   },
   "outputs": [],
   "source": [
    "# Try regression on diabetes data set \n",
    "\n",
    "Xd, Yd = load_diabetes(return_X_y=True)\n",
    "\n",
    "feature_names_d = load_diabetes()['feature_names']\n",
    "\n",
    "Xd_train, Xd_test, Yd_train, Yd_test = train_test_split( Xd, Yd, test_size=0.33, random_state= 42)\n",
    "\n",
    "Xd_scaler = StandardScaler()\n",
    "Xd_scaler.fit(Xd_train)\n",
    "\n",
    "Xd_train_s = Xd_scaler.transform(Xd_train)\n",
    "Xd_test_s = Xd_scaler.transform(Xd_test)\n",
    "\n",
    "\n",
    "Yd_train_s = (Yd_train - Yd_train.mean())/Yd_train.std() \n",
    "Yd_test_s = (Yd_test - Yd_train.mean())/Yd_test.std()\n",
    "\n",
    "\n",
    "\n",
    "alg = 'adam' # adam, lbfgs, sgd\n",
    "\n",
    "d_MLP = MLPRegressor( hidden_layer_sizes= (40, 40, 40), activation = actf, alpha = lmbd, verbose= True, random_state = rng, solver = alg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbwPaIA_iMY5",
    "outputId": "1223bdab-f3c6-4f0e-8a2d-f6d07ac91c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14627.10591006\n",
      "Iteration 2, loss = 14591.10773286\n",
      "Iteration 3, loss = 14555.13485951\n",
      "Iteration 4, loss = 14518.26629730\n",
      "Iteration 5, loss = 14481.28271244\n",
      "Iteration 6, loss = 14443.10381485\n",
      "Iteration 7, loss = 14404.73350342\n",
      "Iteration 8, loss = 14364.19336593\n",
      "Iteration 9, loss = 14322.24883648\n",
      "Iteration 10, loss = 14280.25150325\n",
      "Iteration 11, loss = 14236.61935802\n",
      "Iteration 12, loss = 14191.02743132\n",
      "Iteration 13, loss = 14144.30108619\n",
      "Iteration 14, loss = 14096.48177224\n",
      "Iteration 15, loss = 14047.67060406\n",
      "Iteration 16, loss = 13999.16609548\n",
      "Iteration 17, loss = 13948.83367123\n",
      "Iteration 18, loss = 13898.81359704\n",
      "Iteration 19, loss = 13848.04385379\n",
      "Iteration 20, loss = 13799.23753175\n",
      "Iteration 21, loss = 13750.75499578\n",
      "Iteration 22, loss = 13703.12045587\n",
      "Iteration 23, loss = 13656.90993499\n",
      "Iteration 24, loss = 13612.08423099\n",
      "Iteration 25, loss = 13568.29016273\n",
      "Iteration 26, loss = 13526.23814089\n",
      "Iteration 27, loss = 13486.38315508\n",
      "Iteration 28, loss = 13447.90958118\n",
      "Iteration 29, loss = 13410.72328814\n",
      "Iteration 30, loss = 13375.75237707\n",
      "Iteration 31, loss = 13342.85611967\n",
      "Iteration 32, loss = 13310.73110936\n",
      "Iteration 33, loss = 13280.59123041\n",
      "Iteration 34, loss = 13251.68826206\n",
      "Iteration 35, loss = 13224.43011898\n",
      "Iteration 36, loss = 13198.37850497\n",
      "Iteration 37, loss = 13173.40757818\n",
      "Iteration 38, loss = 13149.73502515\n",
      "Iteration 39, loss = 13126.81677754\n",
      "Iteration 40, loss = 13104.97246432\n",
      "Iteration 41, loss = 13083.85691914\n",
      "Iteration 42, loss = 13063.49185735\n",
      "Iteration 43, loss = 13043.42936874\n",
      "Iteration 44, loss = 13024.60589480\n",
      "Iteration 45, loss = 13005.52582457\n",
      "Iteration 46, loss = 12987.78796448\n",
      "Iteration 47, loss = 12969.73518181\n",
      "Iteration 48, loss = 12952.07109492\n",
      "Iteration 49, loss = 12935.01571807\n",
      "Iteration 50, loss = 12918.43855476\n",
      "Iteration 51, loss = 12901.50729008\n",
      "Iteration 52, loss = 12885.36245876\n",
      "Iteration 53, loss = 12869.36627129\n",
      "Iteration 54, loss = 12853.55571932\n",
      "Iteration 55, loss = 12837.56407591\n",
      "Iteration 56, loss = 12821.96252909\n",
      "Iteration 57, loss = 12806.93422358\n",
      "Iteration 58, loss = 12791.65182038\n",
      "Iteration 59, loss = 12776.51640104\n",
      "Iteration 60, loss = 12761.74160300\n",
      "Iteration 61, loss = 12746.85601347\n",
      "Iteration 62, loss = 12732.41892565\n",
      "Iteration 63, loss = 12717.64097919\n",
      "Iteration 64, loss = 12703.41255193\n",
      "Iteration 65, loss = 12689.20422597\n",
      "Iteration 66, loss = 12674.94958332\n",
      "Iteration 67, loss = 12660.75665950\n",
      "Iteration 68, loss = 12646.83217449\n",
      "Iteration 69, loss = 12633.00546617\n",
      "Iteration 70, loss = 12619.19271409\n",
      "Iteration 71, loss = 12605.35662212\n",
      "Iteration 72, loss = 12591.58998804\n",
      "Iteration 73, loss = 12578.10540002\n",
      "Iteration 74, loss = 12564.65817928\n",
      "Iteration 75, loss = 12551.02533584\n",
      "Iteration 76, loss = 12537.66938718\n",
      "Iteration 77, loss = 12524.27961744\n",
      "Iteration 78, loss = 12511.07509863\n",
      "Iteration 79, loss = 12497.82398040\n",
      "Iteration 80, loss = 12484.52107585\n",
      "Iteration 81, loss = 12471.59781844\n",
      "Iteration 82, loss = 12458.54519031\n",
      "Iteration 83, loss = 12445.32526370\n",
      "Iteration 84, loss = 12432.50780322\n",
      "Iteration 85, loss = 12419.53611900\n",
      "Iteration 86, loss = 12406.57669933\n",
      "Iteration 87, loss = 12393.58540130\n",
      "Iteration 88, loss = 12381.07292993\n",
      "Iteration 89, loss = 12367.97551874\n",
      "Iteration 90, loss = 12355.52235034\n",
      "Iteration 91, loss = 12342.67592119\n",
      "Iteration 92, loss = 12329.92652993\n",
      "Iteration 93, loss = 12317.40025729\n",
      "Iteration 94, loss = 12304.80867461\n",
      "Iteration 95, loss = 12292.22056261\n",
      "Iteration 96, loss = 12279.78318231\n",
      "Iteration 97, loss = 12267.41069008\n",
      "Iteration 98, loss = 12254.91856828\n",
      "Iteration 99, loss = 12242.48459937\n",
      "Iteration 100, loss = 12230.23897019\n",
      "Iteration 101, loss = 12217.87035964\n",
      "Iteration 102, loss = 12205.52963304\n",
      "Iteration 103, loss = 12193.26055566\n",
      "Iteration 104, loss = 12180.92353657\n",
      "Iteration 105, loss = 12168.76848785\n",
      "Iteration 106, loss = 12156.54104083\n",
      "Iteration 107, loss = 12144.34403035\n",
      "Iteration 108, loss = 12132.12407983\n",
      "Iteration 109, loss = 12119.85473876\n",
      "Iteration 110, loss = 12108.10770623\n",
      "Iteration 111, loss = 12095.66964082\n",
      "Iteration 112, loss = 12083.70008828\n",
      "Iteration 113, loss = 12071.89377609\n",
      "Iteration 114, loss = 12059.73579438\n",
      "Iteration 115, loss = 12047.73978651\n",
      "Iteration 116, loss = 12035.90194129\n",
      "Iteration 117, loss = 12023.95499318\n",
      "Iteration 118, loss = 12012.22567152\n",
      "Iteration 119, loss = 12000.19269659\n",
      "Iteration 120, loss = 11988.50938114\n",
      "Iteration 121, loss = 11976.60043567\n",
      "Iteration 122, loss = 11964.89149403\n",
      "Iteration 123, loss = 11953.07409491\n",
      "Iteration 124, loss = 11941.45997944\n",
      "Iteration 125, loss = 11929.67159895\n",
      "Iteration 126, loss = 11917.87124962\n",
      "Iteration 127, loss = 11906.39103484\n",
      "Iteration 128, loss = 11894.97679102\n",
      "Iteration 129, loss = 11882.88115616\n",
      "Iteration 130, loss = 11871.54965162\n",
      "Iteration 131, loss = 11860.00193646\n",
      "Iteration 132, loss = 11848.65609690\n",
      "Iteration 133, loss = 11836.89774460\n",
      "Iteration 134, loss = 11825.63535286\n",
      "Iteration 135, loss = 11814.03256037\n",
      "Iteration 136, loss = 11802.81407076\n",
      "Iteration 137, loss = 11791.40622312\n",
      "Iteration 138, loss = 11780.00161602\n",
      "Iteration 139, loss = 11768.82640017\n",
      "Iteration 140, loss = 11757.32253918\n",
      "Iteration 141, loss = 11746.08101840\n",
      "Iteration 142, loss = 11734.60648760\n",
      "Iteration 143, loss = 11723.44322517\n",
      "Iteration 144, loss = 11712.21294265\n",
      "Iteration 145, loss = 11701.07099718\n",
      "Iteration 146, loss = 11689.49006148\n",
      "Iteration 147, loss = 11678.41006165\n",
      "Iteration 148, loss = 11667.01736143\n",
      "Iteration 149, loss = 11656.02001357\n",
      "Iteration 150, loss = 11644.71369296\n",
      "Iteration 151, loss = 11633.58134773\n",
      "Iteration 152, loss = 11622.40942026\n",
      "Iteration 153, loss = 11611.32459958\n",
      "Iteration 154, loss = 11600.17607963\n",
      "Iteration 155, loss = 11589.06152811\n",
      "Iteration 156, loss = 11577.97673822\n",
      "Iteration 157, loss = 11566.91165396\n",
      "Iteration 158, loss = 11555.84876919\n",
      "Iteration 159, loss = 11544.76001652\n",
      "Iteration 160, loss = 11533.64915894\n",
      "Iteration 161, loss = 11522.74522302\n",
      "Iteration 162, loss = 11511.68693667\n",
      "Iteration 163, loss = 11500.57012464\n",
      "Iteration 164, loss = 11489.58355276\n",
      "Iteration 165, loss = 11478.65236119\n",
      "Iteration 166, loss = 11467.62352145\n",
      "Iteration 167, loss = 11456.84636426\n",
      "Iteration 168, loss = 11445.70306202\n",
      "Iteration 169, loss = 11434.96213533\n",
      "Iteration 170, loss = 11423.97633833\n",
      "Iteration 171, loss = 11413.21323084\n",
      "Iteration 172, loss = 11402.27603505\n",
      "Iteration 173, loss = 11391.38482439\n",
      "Iteration 174, loss = 11380.64986789\n",
      "Iteration 175, loss = 11369.91518809\n",
      "Iteration 176, loss = 11359.00734605\n",
      "Iteration 177, loss = 11348.46800918\n",
      "Iteration 178, loss = 11337.55487482\n",
      "Iteration 179, loss = 11326.67290100\n",
      "Iteration 180, loss = 11316.13680814\n",
      "Iteration 181, loss = 11305.42346063\n",
      "Iteration 182, loss = 11294.62931892\n",
      "Iteration 183, loss = 11284.11634505\n",
      "Iteration 184, loss = 11273.16501477\n",
      "Iteration 185, loss = 11262.75977803\n",
      "Iteration 186, loss = 11252.04485867\n",
      "Iteration 187, loss = 11241.39340233\n",
      "Iteration 188, loss = 11230.79777042\n",
      "Iteration 189, loss = 11220.23933738\n",
      "Iteration 190, loss = 11209.60755770\n",
      "Iteration 191, loss = 11198.98961278\n",
      "Iteration 192, loss = 11188.56085768\n",
      "Iteration 193, loss = 11178.05974389\n",
      "Iteration 194, loss = 11167.49161300\n",
      "Iteration 195, loss = 11157.11113284\n",
      "Iteration 196, loss = 11146.58337258\n",
      "Iteration 197, loss = 11135.96482234\n",
      "Iteration 198, loss = 11125.72590199\n",
      "Iteration 199, loss = 11115.45355290\n",
      "Iteration 200, loss = 11104.92916729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', alpha=0.001, hidden_layer_sizes=(40, 40, 40),\n",
       "             random_state=10, verbose=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit \n",
    "\n",
    "d_MLP.fit(Xd_train, Yd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTU3pmnoiqyw",
    "outputId": "b5eb9746-017c-4715-d112-2c176184b0a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression train MSE: 520.2249881563903\n",
      "Regrassion test MSE: 508.46545623660154\n"
     ]
    }
   ],
   "source": [
    "# report MSEs \n",
    "\n",
    "d_train_MSE = mean_squared_error( Yd_train_s, d_MLP.predict(Xd_train_s) )\n",
    "d_test_MSE = mean_squared_error( Yd_test_s, d_MLP.predict(Xd_test_s) )\n",
    "\n",
    "print('Regression train MSE:', d_train_MSE)\n",
    "print('Regrassion test MSE:', d_test_MSE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AMATH582-Lec27.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
